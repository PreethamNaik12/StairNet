{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ffc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from smart_open import open\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b0833dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists in /home/ec2-user/SageMaker/data/images. Skipping download and extraction.\n",
      "Data check complete. Images are available in /home/ec2-user/SageMaker/data/images\n"
     ]
    }
   ],
   "source": [
    "# Set up the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the S3 bucket and key\n",
    "bucket_name = 'ieee-dataport'\n",
    "object_key = 'data/7540/StairNet.zip'\n",
    "\n",
    "# Define the directory to save images\n",
    "local_image_dir = os.path.expanduser('~/SageMaker/data/images')\n",
    "\n",
    "# Function to check if data already exists\n",
    "def data_exists(local_dir):\n",
    "    # Check for the presence of any image files in the directory\n",
    "    images = glob.glob(os.path.join(local_dir, '*'))\n",
    "    if images:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to stream and extract images\n",
    "def stream_and_extract_images(bucket, key, local_dir):\n",
    "    buffer_size = 10 * 1024 * 1024  # 10 MB buffer size for reading chunks\n",
    "\n",
    "    # Use smart_open to stream the zip file\n",
    "    with open(f's3://{bucket}/{key}', 'rb') as s3_file:\n",
    "        # BufferedReader with a buffer size\n",
    "        with zipfile.ZipFile(io.BufferedReader(s3_file, buffer_size=buffer_size)) as z:\n",
    "            # Iterate over the files in the zip\n",
    "            for file_info in z.infolist():\n",
    "                if file_info.filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    try:\n",
    "                        with z.open(file_info) as image_file:\n",
    "                            img_save_path = os.path.join(local_dir, os.path.basename(file_info.filename))\n",
    "                            # Save only if the file doesn't exist\n",
    "                            if not os.path.exists(img_save_path):\n",
    "                                print(f\"Extracting {file_info.filename}...\")\n",
    "                                with open(img_save_path, 'wb') as f:\n",
    "                                    f.write(image_file.read())\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to extract {file_info.filename}: {e}\")\n",
    "\n",
    "# Create the local directory if it does not exist\n",
    "os.makedirs(local_image_dir, exist_ok=True)\n",
    "\n",
    "# Check if the data already exists\n",
    "if data_exists(local_image_dir):\n",
    "    print(f\"Data already exists in {local_image_dir}. Skipping download and extraction.\")\n",
    "else:\n",
    "    print(f\"Data does not exist in {local_image_dir}. Downloading and extracting...\")\n",
    "    # Stream and extract images\n",
    "    stream_and_extract_images(bucket_name, object_key, local_image_dir)\n",
    "\n",
    "print(f\"Data check complete. Images are available in {local_image_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a080ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images ending with 'LG': 442360\n",
      "Number of images ending with 'LG-IS': 15888\n",
      "Number of images ending with 'IS': 48179\n",
      "Number of images ending with 'IS-LG': 9025\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the images\n",
    "image_directory = '/home/ec2-user/SageMaker/data/images'\n",
    "\n",
    "# Initialize counters for each suffix\n",
    "lg_count = 0\n",
    "lg_is_count = 0\n",
    "is_count = 0\n",
    "is_lg_count = 0\n",
    "\n",
    "# List all files in the directory\n",
    "for filename in os.listdir(image_directory):\n",
    "    # Check and count for each suffix\n",
    "    if filename.endswith(' LG.jpg'):\n",
    "        lg_count += 1\n",
    "    elif filename.endswith(' LGIS.jpg'):\n",
    "        lg_is_count += 1\n",
    "    elif filename.endswith(' IS.jpg'):\n",
    "        is_count += 1\n",
    "    elif filename.endswith(' ISLG.jpg'):\n",
    "        is_lg_count += 1\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Number of images ending with 'LG': {lg_count}\")\n",
    "print(f\"Number of images ending with 'LG-IS': {lg_is_count}\")\n",
    "print(f\"Number of images ending with 'IS': {is_count}\")\n",
    "print(f\"Number of images ending with 'IS-LG': {is_lg_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d75cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Rest of your code will go here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e083e44",
   "metadata": {},
   "source": [
    "## Step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57725836",
   "metadata": {},
   "source": [
    "- First, let's define the dataset structure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb120e9",
   "metadata": {},
   "source": [
    "dataset_structure = {\n",
    "    'IS': 48179,\n",
    "    'ISLG': 9025,\n",
    "    'LG': 442360,\n",
    "    'LGIS': 15888\n",
    "}\n",
    "\n",
    "total_samples = sum(dataset_structure.values())\n",
    "print(f\"Total samples: {total_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8c403",
   "metadata": {},
   "source": [
    "- Now, let's create functions to split the data according to the given percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_dataset(dataset_structure, train_pct=0.895, val_pct=0.035, test_pct=0.07):\n",
    "#     train_set, val_set, test_set = {}, {}, {}\n",
    "    \n",
    "#     for class_name, total_samples in dataset_structure.items():\n",
    "#         train_samples = int(total_samples * train_pct)\n",
    "#         val_samples = int(total_samples * val_pct)\n",
    "#         test_samples = total_samples - train_samples - val_samples\n",
    "        \n",
    "#         train_set[class_name] = train_samples\n",
    "#         val_set[class_name] = val_samples\n",
    "#         test_set[class_name] = test_samples\n",
    "    \n",
    "#     return train_set, val_set, test_set\n",
    "\n",
    "# train_set, val_set, test_set = split_dataset(dataset_structure)\n",
    "\n",
    "# print(\"Training set:\", train_set)\n",
    "# print(\"Validation set:\", val_set)\n",
    "# print(\"Test set:\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ec6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file_list, train_pct=0.895, val_pct=0.035, test_pct=0.07):\n",
    "    random.shuffle(file_list)\n",
    "    total = len(file_list)\n",
    "    train_end = int(total * train_pct)\n",
    "    val_end = train_end + int(total * val_pct)\n",
    "    \n",
    "    return file_list[:train_end], file_list[train_end:val_end], file_list[val_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5ed43",
   "metadata": {},
   "source": [
    "- For loading the data, we'll need to create a custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ca17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StairNetDataset(Dataset):\n",
    "    def __init__(self, root_dir, file_list, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {'IS': 0, 'ISLG': 1, 'LG': 2, 'LGIS': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.file_list[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Extract label from filename\n",
    "        parts = img_name.split()\n",
    "        label = parts[-1].split('.')[0]  # Get the last part before the file extension\n",
    "        \n",
    "        # Handle the case where the label might be two words (e.g., 'LG IS')\n",
    "        if len(parts) > 2 and parts[-2] in ['LG', 'IS']:\n",
    "            label = f\"{parts[-2]} {label}\"\n",
    "        \n",
    "        label_idx = self.class_to_idx[label]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae8b87",
   "metadata": {},
   "source": [
    "- creat a stratified dataset\n",
    "- setup the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33a9fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 461328\n",
      "Number of validation samples: 18039\n",
      "Number of test samples: 36085\n"
     ]
    }
   ],
   "source": [
    "def split_data_stratified(file_list, train_pct=0.895, val_pct=0.035, test_pct=0.07):\n",
    "    # Group files by class\n",
    "    class_files = defaultdict(list)\n",
    "    for file in file_list:\n",
    "        parts = file.split()\n",
    "        label = parts[-1].split('.')[0]\n",
    "        if len(parts) > 2 and parts[-2] in ['LG', 'IS']:\n",
    "            label = f\"{parts[-2]} {label}\"\n",
    "        class_files[label].append(file)\n",
    "    \n",
    "    train_files, val_files, test_files = [], [], []\n",
    "    \n",
    "    for class_label, files in class_files.items():\n",
    "        random.shuffle(files)\n",
    "        n_files = len(files)\n",
    "        n_train = int(n_files * train_pct)\n",
    "        n_val = int(n_files * val_pct)\n",
    "        \n",
    "        train_files.extend(files[:n_train])\n",
    "        val_files.extend(files[n_train:n_train+n_val])\n",
    "        test_files.extend(files[n_train+n_val:])\n",
    "    \n",
    "    random.shuffle(train_files)\n",
    "    random.shuffle(val_files)\n",
    "    random.shuffle(test_files)\n",
    "    \n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Set up data\n",
    "root_dir = '/home/ec2-user/SageMaker/data/images'  \n",
    "all_files = os.listdir(root_dir)\n",
    "image_files = [f for f in all_files if f.endswith('.jpg')]\n",
    "\n",
    "# Split data\n",
    "train_files, val_files, test_files = split_data_stratified(image_files)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = StairNetDataset(root_dir, train_files, transform=train_transform)\n",
    "val_dataset = StairNetDataset(root_dir, val_files, transform=val_transform)\n",
    "test_dataset = StairNetDataset(root_dir, test_files, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3353ef5",
   "metadata": {},
   "source": [
    "- check the distribution (caution: takes very very long time to count! so run this if you have a lot of time availale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print class distribution in each split\n",
    "\n",
    "# def print_class_distribution(dataset):\n",
    "#     class_counts = defaultdict(int)\n",
    "#     for _, label in dataset:\n",
    "#         class_counts[label] += 1\n",
    "#     total = sum(class_counts.values())\n",
    "#     for label, count in class_counts.items():\n",
    "#         print(f\"Class {label}: {count} ({count/total:.2%})\")\n",
    "\n",
    "# print(\"\\nTraining set class distribution:\")\n",
    "# print_class_distribution(train_dataset)\n",
    "# print(\"\\nValidation set class distribution:\")\n",
    "# print_class_distribution(val_dataset)\n",
    "# print(\"\\nTest set class distribution:\")\n",
    "# print_class_distribution(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a74189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=4, dropout_rate=0.2):\n",
    "        super(CustomMobileNetV2, self).__init__()\n",
    "        \n",
    "        # Load pretrained MobileNetV2\n",
    "        self.model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Freeze first 5 layers\n",
    "        for i, param in enumerate(self.model.features[:5].parameters()):\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Modify the classifier\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a00b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45abde76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "             ReLU6-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "             ReLU6-6         [-1, 32, 112, 112]               0\n",
      "            Conv2d-7         [-1, 16, 112, 112]             512\n",
      "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
      "  InvertedResidual-9         [-1, 16, 112, 112]               0\n",
      "           Conv2d-10         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-11         [-1, 96, 112, 112]             192\n",
      "            ReLU6-12         [-1, 96, 112, 112]               0\n",
      "           Conv2d-13           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-14           [-1, 96, 56, 56]             192\n",
      "            ReLU6-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 56, 56]             288\n",
      "            ReLU6-21          [-1, 144, 56, 56]               0\n",
      "           Conv2d-22          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 56, 56]             288\n",
      "            ReLU6-24          [-1, 144, 56, 56]               0\n",
      "           Conv2d-25           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-27           [-1, 24, 56, 56]               0\n",
      "           Conv2d-28          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 56, 56]             288\n",
      "            ReLU6-30          [-1, 144, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 28, 28]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 28, 28]             288\n",
      "            ReLU6-33          [-1, 144, 28, 28]               0\n",
      "           Conv2d-34           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-36           [-1, 32, 28, 28]               0\n",
      "           Conv2d-37          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 28, 28]             384\n",
      "            ReLU6-39          [-1, 192, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
      "            ReLU6-42          [-1, 192, 28, 28]               0\n",
      "           Conv2d-43           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-45           [-1, 32, 28, 28]               0\n",
      "           Conv2d-46          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 28, 28]             384\n",
      "            ReLU6-48          [-1, 192, 28, 28]               0\n",
      "           Conv2d-49          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
      "            ReLU6-51          [-1, 192, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-54           [-1, 32, 28, 28]               0\n",
      "           Conv2d-55          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 28, 28]             384\n",
      "            ReLU6-57          [-1, 192, 28, 28]               0\n",
      "           Conv2d-58          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-59          [-1, 192, 14, 14]             384\n",
      "            ReLU6-60          [-1, 192, 14, 14]               0\n",
      "           Conv2d-61           [-1, 64, 14, 14]          12,288\n",
      "      BatchNorm2d-62           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-63           [-1, 64, 14, 14]               0\n",
      "           Conv2d-64          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-65          [-1, 384, 14, 14]             768\n",
      "            ReLU6-66          [-1, 384, 14, 14]               0\n",
      "           Conv2d-67          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
      "            ReLU6-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-71           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-72           [-1, 64, 14, 14]               0\n",
      "           Conv2d-73          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
      "            ReLU6-75          [-1, 384, 14, 14]               0\n",
      "           Conv2d-76          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-77          [-1, 384, 14, 14]             768\n",
      "            ReLU6-78          [-1, 384, 14, 14]               0\n",
      "           Conv2d-79           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-80           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-81           [-1, 64, 14, 14]               0\n",
      "           Conv2d-82          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-83          [-1, 384, 14, 14]             768\n",
      "            ReLU6-84          [-1, 384, 14, 14]               0\n",
      "           Conv2d-85          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
      "            ReLU6-87          [-1, 384, 14, 14]               0\n",
      "           Conv2d-88           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-89           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-90           [-1, 64, 14, 14]               0\n",
      "           Conv2d-91          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
      "            ReLU6-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-95          [-1, 384, 14, 14]             768\n",
      "            ReLU6-96          [-1, 384, 14, 14]               0\n",
      "           Conv2d-97           [-1, 96, 14, 14]          36,864\n",
      "      BatchNorm2d-98           [-1, 96, 14, 14]             192\n",
      " InvertedResidual-99           [-1, 96, 14, 14]               0\n",
      "          Conv2d-100          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-102          [-1, 576, 14, 14]               0\n",
      "          Conv2d-103          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-105          [-1, 576, 14, 14]               0\n",
      "          Conv2d-106           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-107           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-108           [-1, 96, 14, 14]               0\n",
      "          Conv2d-109          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-111          [-1, 576, 14, 14]               0\n",
      "          Conv2d-112          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-114          [-1, 576, 14, 14]               0\n",
      "          Conv2d-115           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-116           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-117           [-1, 96, 14, 14]               0\n",
      "          Conv2d-118          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-120          [-1, 576, 14, 14]               0\n",
      "          Conv2d-121            [-1, 576, 7, 7]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n",
      "           ReLU6-123            [-1, 576, 7, 7]               0\n",
      "          Conv2d-124            [-1, 160, 7, 7]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-126            [-1, 160, 7, 7]               0\n",
      "          Conv2d-127            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-129            [-1, 960, 7, 7]               0\n",
      "          Conv2d-130            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-132            [-1, 960, 7, 7]               0\n",
      "          Conv2d-133            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-135            [-1, 160, 7, 7]               0\n",
      "          Conv2d-136            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-138            [-1, 960, 7, 7]               0\n",
      "          Conv2d-139            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-141            [-1, 960, 7, 7]               0\n",
      "          Conv2d-142            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-144            [-1, 160, 7, 7]               0\n",
      "          Conv2d-145            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-147            [-1, 960, 7, 7]               0\n",
      "          Conv2d-148            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-150            [-1, 960, 7, 7]               0\n",
      "          Conv2d-151            [-1, 320, 7, 7]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 7, 7]             640\n",
      "InvertedResidual-153            [-1, 320, 7, 7]               0\n",
      "          Conv2d-154           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n",
      "           ReLU6-156           [-1, 1280, 7, 7]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                    [-1, 4]           5,124\n",
      "     MobileNetV2-159                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 2,228,996\n",
      "Trainable params: 2,203,204\n",
      "Non-trainable params: 25,792\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 152.86\n",
      "Params size (MB): 8.50\n",
      "Estimated Total Size (MB): 161.94\n",
      "----------------------------------------------------------------\n",
      "Number of trainable parameters: 2203204\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = CustomMobileNetV2(num_classes=4, dropout_rate=0.2)\n",
    "\n",
    "# Print model summary\n",
    "from torchsummary import summary\n",
    "summary(model, (3, 224, 224), device='cpu')\n",
    "\n",
    "# Verify number of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb5c3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # for optimization\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   4%|▍         | 151/3605 [23:12<8:46:30,  9.15s/it]"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        #optimization\n",
    "        # Add this line to update the scheduler\n",
    "#         scheduler.step(val_loss)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f'Best validation accuracy: {best_val_accuracy:.4f}')\n",
    "\n",
    "# Use the function\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b1f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
